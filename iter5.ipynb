{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, FunctionTransformer, LabelEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold, train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from pycaret.classification import *\n",
    "from function.data_engineering import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_odds(win_odds, draw_odds):\n",
    "    loss_odds = 1 / (1 / win_odds + 1 / draw_odds)\n",
    "    return loss_odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('penyisihan-satria-data-itb-2023/train.csv')\n",
    "test = pd.read_csv('penyisihan-satria-data-itb-2023/test.csv')\n",
    "data.index = data.id\n",
    "test.index = test.id\n",
    "data = data.iloc[:,1:]\n",
    "test = test.iloc[:,1:]\n",
    "or_cols = [c for c in data.columns if c != 'target']\n",
    "int_cols = [c for c in data.columns if data[c].dtype=='int64' and c != 'target']\n",
    "float_cols = [c for c in data.columns if data[c].dtype=='float64' and c != 'target']\n",
    "h_columns = [c for c in data.columns if c[-1]=='h']\n",
    "d_columns = [c for c in data.columns if c[-1]=='d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = (data['target']).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 600 entries, 1 to 600\n",
      "Data columns (total 36 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   hs           600 non-null    int64   \n",
      " 1   hst          600 non-null    int64   \n",
      " 2   hf           600 non-null    int64   \n",
      " 3   hc           600 non-null    int64   \n",
      " 4   hy           600 non-null    int64   \n",
      " 5   hr           600 non-null    int64   \n",
      " 6   b365h        600 non-null    float64 \n",
      " 7   b365d        600 non-null    float64 \n",
      " 8   bwh          600 non-null    float64 \n",
      " 9   bwd          600 non-null    float64 \n",
      " 10  iwh          600 non-null    float64 \n",
      " 11  iwd          600 non-null    float64 \n",
      " 12  psh          600 non-null    float64 \n",
      " 13  psd          600 non-null    float64 \n",
      " 14  whh          600 non-null    float64 \n",
      " 15  whd          600 non-null    float64 \n",
      " 16  vch          600 non-null    float64 \n",
      " 17  vcd          600 non-null    float64 \n",
      " 18  bb_mx_h      600 non-null    float64 \n",
      " 19  bb_av_h      600 non-null    float64 \n",
      " 20  bb_mx_d      600 non-null    float64 \n",
      " 21  bb_av_d      600 non-null    float64 \n",
      " 22  bb_ou        600 non-null    int64   \n",
      " 23  bb_mx_2_5    600 non-null    float64 \n",
      " 24  bb_av_2_5    600 non-null    float64 \n",
      " 25  bb_mx_2_5_1  600 non-null    float64 \n",
      " 26  bb_av_2_5_1  600 non-null    float64 \n",
      " 27  bb_ah        600 non-null    int64   \n",
      " 28  bb_a_hh      600 non-null    float64 \n",
      " 29  bb_mx_ahh    600 non-null    float64 \n",
      " 30  bb_av_ahh    600 non-null    float64 \n",
      " 31  bb_mx_aha    600 non-null    float64 \n",
      " 32  bb_av_aha    600 non-null    float64 \n",
      " 33  psch         600 non-null    float64 \n",
      " 34  pscd         600 non-null    float64 \n",
      " 35  target       600 non-null    category\n",
      "dtypes: category(1), float64(27), int64(8)\n",
      "memory usage: 169.5 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [c for c in data.columns if c != 'target']\n",
    "scale = StandardScaler()\n",
    "scale.fit(data[X])\n",
    "data[X] = scale.transform(data[X])\n",
    "test[X] = scale.transform(test[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = data.copy()\n",
    "tmp_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs</th>\n",
       "      <th>hst</th>\n",
       "      <th>hf</th>\n",
       "      <th>hc</th>\n",
       "      <th>hy</th>\n",
       "      <th>hr</th>\n",
       "      <th>b365h</th>\n",
       "      <th>b365d</th>\n",
       "      <th>bwh</th>\n",
       "      <th>bwd</th>\n",
       "      <th>iwh</th>\n",
       "      <th>iwd</th>\n",
       "      <th>psh</th>\n",
       "      <th>psd</th>\n",
       "      <th>whh</th>\n",
       "      <th>whd</th>\n",
       "      <th>vch</th>\n",
       "      <th>vcd</th>\n",
       "      <th>bb_mx_h</th>\n",
       "      <th>bb_av_h</th>\n",
       "      <th>bb_mx_d</th>\n",
       "      <th>bb_av_d</th>\n",
       "      <th>bb_ou</th>\n",
       "      <th>bb_mx_2_5</th>\n",
       "      <th>bb_av_2_5</th>\n",
       "      <th>bb_mx_2_5_1</th>\n",
       "      <th>bb_av_2_5_1</th>\n",
       "      <th>bb_ah</th>\n",
       "      <th>bb_a_hh</th>\n",
       "      <th>bb_mx_ahh</th>\n",
       "      <th>bb_av_ahh</th>\n",
       "      <th>bb_mx_aha</th>\n",
       "      <th>bb_av_aha</th>\n",
       "      <th>psch</th>\n",
       "      <th>pscd</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.524832</td>\n",
       "      <td>0.434034</td>\n",
       "      <td>-1.012809</td>\n",
       "      <td>1.429838</td>\n",
       "      <td>0.430123</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.418164</td>\n",
       "      <td>-0.540222</td>\n",
       "      <td>-0.430541</td>\n",
       "      <td>-0.505686</td>\n",
       "      <td>-0.453979</td>\n",
       "      <td>-0.513666</td>\n",
       "      <td>-0.438482</td>\n",
       "      <td>-0.509535</td>\n",
       "      <td>-0.461495</td>\n",
       "      <td>-0.477347</td>\n",
       "      <td>-0.425416</td>\n",
       "      <td>-0.539942</td>\n",
       "      <td>-0.407310</td>\n",
       "      <td>-0.422638</td>\n",
       "      <td>-0.531913</td>\n",
       "      <td>-0.524689</td>\n",
       "      <td>0.940543</td>\n",
       "      <td>0.483659</td>\n",
       "      <td>0.507003</td>\n",
       "      <td>-0.620425</td>\n",
       "      <td>-0.630216</td>\n",
       "      <td>1.007366</td>\n",
       "      <td>-0.363559</td>\n",
       "      <td>1.265339</td>\n",
       "      <td>1.400863</td>\n",
       "      <td>-1.311788</td>\n",
       "      <td>-1.296572</td>\n",
       "      <td>-0.435985</td>\n",
       "      <td>-0.585915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.181208</td>\n",
       "      <td>0.067245</td>\n",
       "      <td>-1.631319</td>\n",
       "      <td>0.749503</td>\n",
       "      <td>-0.386567</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.673929</td>\n",
       "      <td>1.071365</td>\n",
       "      <td>-0.704861</td>\n",
       "      <td>1.005207</td>\n",
       "      <td>-0.764299</td>\n",
       "      <td>1.103441</td>\n",
       "      <td>-0.687074</td>\n",
       "      <td>0.894218</td>\n",
       "      <td>-0.698224</td>\n",
       "      <td>1.074059</td>\n",
       "      <td>-0.678438</td>\n",
       "      <td>0.839573</td>\n",
       "      <td>-0.655619</td>\n",
       "      <td>-0.698576</td>\n",
       "      <td>0.915718</td>\n",
       "      <td>0.940038</td>\n",
       "      <td>0.226207</td>\n",
       "      <td>-1.083331</td>\n",
       "      <td>-1.072879</td>\n",
       "      <td>0.896919</td>\n",
       "      <td>0.995728</td>\n",
       "      <td>-0.038343</td>\n",
       "      <td>-1.351714</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.581379</td>\n",
       "      <td>-0.717649</td>\n",
       "      <td>-0.724723</td>\n",
       "      <td>-0.661560</td>\n",
       "      <td>0.370583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493960</td>\n",
       "      <td>0.434034</td>\n",
       "      <td>-0.703555</td>\n",
       "      <td>0.409335</td>\n",
       "      <td>-0.386567</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.673929</td>\n",
       "      <td>1.071365</td>\n",
       "      <td>-0.696420</td>\n",
       "      <td>0.854118</td>\n",
       "      <td>-0.734745</td>\n",
       "      <td>0.892514</td>\n",
       "      <td>-0.698912</td>\n",
       "      <td>0.861188</td>\n",
       "      <td>-0.715133</td>\n",
       "      <td>1.074059</td>\n",
       "      <td>-0.678438</td>\n",
       "      <td>0.839573</td>\n",
       "      <td>-0.652072</td>\n",
       "      <td>-0.702757</td>\n",
       "      <td>0.915718</td>\n",
       "      <td>0.964249</td>\n",
       "      <td>-0.130962</td>\n",
       "      <td>-1.110822</td>\n",
       "      <td>-1.102688</td>\n",
       "      <td>1.033091</td>\n",
       "      <td>0.974049</td>\n",
       "      <td>1.530221</td>\n",
       "      <td>-1.104676</td>\n",
       "      <td>-0.729603</td>\n",
       "      <td>-0.948326</td>\n",
       "      <td>0.902732</td>\n",
       "      <td>0.876454</td>\n",
       "      <td>-0.703616</td>\n",
       "      <td>0.812835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.040268</td>\n",
       "      <td>2.267979</td>\n",
       "      <td>0.533464</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>-0.386567</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.437251</td>\n",
       "      <td>-0.384435</td>\n",
       "      <td>-0.464304</td>\n",
       "      <td>-0.354596</td>\n",
       "      <td>-0.453979</td>\n",
       "      <td>-0.513666</td>\n",
       "      <td>-0.446374</td>\n",
       "      <td>-0.437971</td>\n",
       "      <td>-0.436131</td>\n",
       "      <td>-0.322206</td>\n",
       "      <td>-0.444584</td>\n",
       "      <td>-0.401991</td>\n",
       "      <td>-0.421499</td>\n",
       "      <td>-0.443543</td>\n",
       "      <td>-0.382158</td>\n",
       "      <td>-0.409690</td>\n",
       "      <td>1.297711</td>\n",
       "      <td>-0.286090</td>\n",
       "      <td>-0.357461</td>\n",
       "      <td>-0.036831</td>\n",
       "      <td>-0.023197</td>\n",
       "      <td>-0.561197</td>\n",
       "      <td>-0.116520</td>\n",
       "      <td>-0.371537</td>\n",
       "      <td>-0.402003</td>\n",
       "      <td>0.200567</td>\n",
       "      <td>0.247420</td>\n",
       "      <td>-0.390105</td>\n",
       "      <td>-0.498493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.009396</td>\n",
       "      <td>0.434034</td>\n",
       "      <td>0.224210</td>\n",
       "      <td>-0.951336</td>\n",
       "      <td>1.246814</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.208207</td>\n",
       "      <td>-0.701381</td>\n",
       "      <td>-0.181543</td>\n",
       "      <td>-0.747428</td>\n",
       "      <td>-0.158436</td>\n",
       "      <td>-0.794902</td>\n",
       "      <td>-0.205674</td>\n",
       "      <td>-0.740741</td>\n",
       "      <td>-0.186720</td>\n",
       "      <td>-0.725572</td>\n",
       "      <td>-0.214564</td>\n",
       "      <td>-0.705484</td>\n",
       "      <td>-0.212210</td>\n",
       "      <td>-0.201052</td>\n",
       "      <td>-0.731586</td>\n",
       "      <td>-0.754688</td>\n",
       "      <td>-0.488130</td>\n",
       "      <td>1.528319</td>\n",
       "      <td>1.520512</td>\n",
       "      <td>-1.067847</td>\n",
       "      <td>-1.085480</td>\n",
       "      <td>-0.038343</td>\n",
       "      <td>0.130519</td>\n",
       "      <td>0.702663</td>\n",
       "      <td>0.799908</td>\n",
       "      <td>-0.879687</td>\n",
       "      <td>-0.781908</td>\n",
       "      <td>-0.141590</td>\n",
       "      <td>-0.735047</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.322148</td>\n",
       "      <td>0.067245</td>\n",
       "      <td>-1.012809</td>\n",
       "      <td>-0.271000</td>\n",
       "      <td>-1.203257</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.570860</td>\n",
       "      <td>-0.164185</td>\n",
       "      <td>-0.574031</td>\n",
       "      <td>-0.203507</td>\n",
       "      <td>-0.651008</td>\n",
       "      <td>0.048806</td>\n",
       "      <td>-0.576589</td>\n",
       "      <td>-0.234289</td>\n",
       "      <td>-0.588314</td>\n",
       "      <td>-0.167066</td>\n",
       "      <td>-0.571095</td>\n",
       "      <td>-0.264039</td>\n",
       "      <td>-0.556295</td>\n",
       "      <td>-0.577331</td>\n",
       "      <td>-0.182485</td>\n",
       "      <td>-0.197849</td>\n",
       "      <td>-0.130962</td>\n",
       "      <td>0.263731</td>\n",
       "      <td>0.298339</td>\n",
       "      <td>-0.484253</td>\n",
       "      <td>-0.478461</td>\n",
       "      <td>0.484512</td>\n",
       "      <td>-0.610598</td>\n",
       "      <td>0.191139</td>\n",
       "      <td>0.144320</td>\n",
       "      <td>-0.231534</td>\n",
       "      <td>-0.267244</td>\n",
       "      <td>-0.600387</td>\n",
       "      <td>-0.092239</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1.181208</td>\n",
       "      <td>2.634768</td>\n",
       "      <td>-1.631319</td>\n",
       "      <td>0.409335</td>\n",
       "      <td>-0.386567</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.719738</td>\n",
       "      <td>2.145757</td>\n",
       "      <td>-0.747064</td>\n",
       "      <td>1.911742</td>\n",
       "      <td>-0.788928</td>\n",
       "      <td>1.595604</td>\n",
       "      <td>-0.734425</td>\n",
       "      <td>1.923636</td>\n",
       "      <td>-0.748952</td>\n",
       "      <td>1.694621</td>\n",
       "      <td>-0.720608</td>\n",
       "      <td>1.667282</td>\n",
       "      <td>-0.691092</td>\n",
       "      <td>-0.744566</td>\n",
       "      <td>1.914084</td>\n",
       "      <td>1.878190</td>\n",
       "      <td>-1.202466</td>\n",
       "      <td>-1.248277</td>\n",
       "      <td>-1.251734</td>\n",
       "      <td>1.324887</td>\n",
       "      <td>1.559389</td>\n",
       "      <td>0.484512</td>\n",
       "      <td>-1.598753</td>\n",
       "      <td>-0.524994</td>\n",
       "      <td>-0.620532</td>\n",
       "      <td>0.470631</td>\n",
       "      <td>0.476160</td>\n",
       "      <td>-0.745673</td>\n",
       "      <td>2.684692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.150336</td>\n",
       "      <td>1.167612</td>\n",
       "      <td>0.533464</td>\n",
       "      <td>-0.271000</td>\n",
       "      <td>1.246814</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.437251</td>\n",
       "      <td>-0.540222</td>\n",
       "      <td>-0.434761</td>\n",
       "      <td>-0.566121</td>\n",
       "      <td>-0.429350</td>\n",
       "      <td>-0.548820</td>\n",
       "      <td>-0.446374</td>\n",
       "      <td>-0.526049</td>\n",
       "      <td>-0.436131</td>\n",
       "      <td>-0.663516</td>\n",
       "      <td>-0.463752</td>\n",
       "      <td>-0.539942</td>\n",
       "      <td>-0.432141</td>\n",
       "      <td>-0.443543</td>\n",
       "      <td>-0.531913</td>\n",
       "      <td>-0.561005</td>\n",
       "      <td>1.297711</td>\n",
       "      <td>1.225918</td>\n",
       "      <td>1.192612</td>\n",
       "      <td>-0.912222</td>\n",
       "      <td>-0.955405</td>\n",
       "      <td>-0.561197</td>\n",
       "      <td>-0.116520</td>\n",
       "      <td>-0.524994</td>\n",
       "      <td>-0.511267</td>\n",
       "      <td>0.416618</td>\n",
       "      <td>0.418975</td>\n",
       "      <td>-0.462748</td>\n",
       "      <td>-0.524206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.524832</td>\n",
       "      <td>0.434034</td>\n",
       "      <td>-0.085045</td>\n",
       "      <td>2.110174</td>\n",
       "      <td>1.246814</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.685382</td>\n",
       "      <td>1.071365</td>\n",
       "      <td>-0.709081</td>\n",
       "      <td>0.854118</td>\n",
       "      <td>-0.774151</td>\n",
       "      <td>1.244059</td>\n",
       "      <td>-0.710749</td>\n",
       "      <td>1.185978</td>\n",
       "      <td>-0.715133</td>\n",
       "      <td>1.074059</td>\n",
       "      <td>-0.693772</td>\n",
       "      <td>1.115476</td>\n",
       "      <td>-0.662713</td>\n",
       "      <td>-0.715300</td>\n",
       "      <td>1.040514</td>\n",
       "      <td>1.006617</td>\n",
       "      <td>-0.130962</td>\n",
       "      <td>-0.286090</td>\n",
       "      <td>-0.387270</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>-0.038343</td>\n",
       "      <td>-1.351714</td>\n",
       "      <td>0.088835</td>\n",
       "      <td>0.144320</td>\n",
       "      <td>-0.339560</td>\n",
       "      <td>-0.267244</td>\n",
       "      <td>-0.703616</td>\n",
       "      <td>0.910542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>2.899329</td>\n",
       "      <td>0.067245</td>\n",
       "      <td>-0.394300</td>\n",
       "      <td>2.110174</td>\n",
       "      <td>-1.203257</td>\n",
       "      <td>-0.213399</td>\n",
       "      <td>-0.528868</td>\n",
       "      <td>-0.271624</td>\n",
       "      <td>-0.540269</td>\n",
       "      <td>-0.203507</td>\n",
       "      <td>-0.552493</td>\n",
       "      <td>-0.302739</td>\n",
       "      <td>-0.548967</td>\n",
       "      <td>-0.305853</td>\n",
       "      <td>-0.554496</td>\n",
       "      <td>-0.291178</td>\n",
       "      <td>-0.532758</td>\n",
       "      <td>-0.319220</td>\n",
       "      <td>-0.520823</td>\n",
       "      <td>-0.543884</td>\n",
       "      <td>-0.282321</td>\n",
       "      <td>-0.258375</td>\n",
       "      <td>0.226207</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>0.059867</td>\n",
       "      <td>-0.328628</td>\n",
       "      <td>-0.348386</td>\n",
       "      <td>1.530221</td>\n",
       "      <td>-0.610598</td>\n",
       "      <td>0.958425</td>\n",
       "      <td>0.963805</td>\n",
       "      <td>-0.987712</td>\n",
       "      <td>-1.010648</td>\n",
       "      <td>-0.550684</td>\n",
       "      <td>-0.380217</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hs       hst        hf        hc        hy        hr     b365h  \\\n",
       "id                                                                          \n",
       "1    1.524832  0.434034 -1.012809  1.429838  0.430123 -0.213399 -0.418164   \n",
       "2    1.181208  0.067245 -1.631319  0.749503 -0.386567 -0.213399 -0.673929   \n",
       "3    0.493960  0.434034 -0.703555  0.409335 -0.386567 -0.213399 -0.673929   \n",
       "4    2.040268  2.267979  0.533464  0.069167 -0.386567 -0.213399 -0.437251   \n",
       "5    1.009396  0.434034  0.224210 -0.951336  1.246814 -0.213399 -0.208207   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596  0.322148  0.067245 -1.012809 -0.271000 -1.203257 -0.213399 -0.570860   \n",
       "597  1.181208  2.634768 -1.631319  0.409335 -0.386567 -0.213399 -0.719738   \n",
       "598  0.150336  1.167612  0.533464 -0.271000  1.246814 -0.213399 -0.437251   \n",
       "599  1.524832  0.434034 -0.085045  2.110174  1.246814 -0.213399 -0.685382   \n",
       "600  2.899329  0.067245 -0.394300  2.110174 -1.203257 -0.213399 -0.528868   \n",
       "\n",
       "        b365d       bwh       bwd       iwh       iwd       psh       psd  \\\n",
       "id                                                                          \n",
       "1   -0.540222 -0.430541 -0.505686 -0.453979 -0.513666 -0.438482 -0.509535   \n",
       "2    1.071365 -0.704861  1.005207 -0.764299  1.103441 -0.687074  0.894218   \n",
       "3    1.071365 -0.696420  0.854118 -0.734745  0.892514 -0.698912  0.861188   \n",
       "4   -0.384435 -0.464304 -0.354596 -0.453979 -0.513666 -0.446374 -0.437971   \n",
       "5   -0.701381 -0.181543 -0.747428 -0.158436 -0.794902 -0.205674 -0.740741   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596 -0.164185 -0.574031 -0.203507 -0.651008  0.048806 -0.576589 -0.234289   \n",
       "597  2.145757 -0.747064  1.911742 -0.788928  1.595604 -0.734425  1.923636   \n",
       "598 -0.540222 -0.434761 -0.566121 -0.429350 -0.548820 -0.446374 -0.526049   \n",
       "599  1.071365 -0.709081  0.854118 -0.774151  1.244059 -0.710749  1.185978   \n",
       "600 -0.271624 -0.540269 -0.203507 -0.552493 -0.302739 -0.548967 -0.305853   \n",
       "\n",
       "          whh       whd       vch       vcd   bb_mx_h   bb_av_h   bb_mx_d  \\\n",
       "id                                                                          \n",
       "1   -0.461495 -0.477347 -0.425416 -0.539942 -0.407310 -0.422638 -0.531913   \n",
       "2   -0.698224  1.074059 -0.678438  0.839573 -0.655619 -0.698576  0.915718   \n",
       "3   -0.715133  1.074059 -0.678438  0.839573 -0.652072 -0.702757  0.915718   \n",
       "4   -0.436131 -0.322206 -0.444584 -0.401991 -0.421499 -0.443543 -0.382158   \n",
       "5   -0.186720 -0.725572 -0.214564 -0.705484 -0.212210 -0.201052 -0.731586   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596 -0.588314 -0.167066 -0.571095 -0.264039 -0.556295 -0.577331 -0.182485   \n",
       "597 -0.748952  1.694621 -0.720608  1.667282 -0.691092 -0.744566  1.914084   \n",
       "598 -0.436131 -0.663516 -0.463752 -0.539942 -0.432141 -0.443543 -0.531913   \n",
       "599 -0.715133  1.074059 -0.693772  1.115476 -0.662713 -0.715300  1.040514   \n",
       "600 -0.554496 -0.291178 -0.532758 -0.319220 -0.520823 -0.543884 -0.282321   \n",
       "\n",
       "      bb_av_d     bb_ou  bb_mx_2_5  bb_av_2_5  bb_mx_2_5_1  bb_av_2_5_1  \\\n",
       "id                                                                        \n",
       "1   -0.524689  0.940543   0.483659   0.507003    -0.620425    -0.630216   \n",
       "2    0.940038  0.226207  -1.083331  -1.072879     0.896919     0.995728   \n",
       "3    0.964249 -0.130962  -1.110822  -1.102688     1.033091     0.974049   \n",
       "4   -0.409690  1.297711  -0.286090  -0.357461    -0.036831    -0.023197   \n",
       "5   -0.754688 -0.488130   1.528319   1.520512    -1.067847    -1.085480   \n",
       "..        ...       ...        ...        ...          ...          ...   \n",
       "596 -0.197849 -0.130962   0.263731   0.298339    -0.484253    -0.478461   \n",
       "597  1.878190 -1.202466  -1.248277  -1.251734     1.324887     1.559389   \n",
       "598 -0.561005  1.297711   1.225918   1.192612    -0.912222    -0.955405   \n",
       "599  1.006617 -0.130962  -0.286090  -0.387270     0.002075     0.041841   \n",
       "600 -0.258375  0.226207   0.016311   0.059867    -0.328628    -0.348386   \n",
       "\n",
       "        bb_ah   bb_a_hh  bb_mx_ahh  bb_av_ahh  bb_mx_aha  bb_av_aha      psch  \\\n",
       "id                                                                              \n",
       "1    1.007366 -0.363559   1.265339   1.400863  -1.311788  -1.296572 -0.435985   \n",
       "2   -0.038343 -1.351714   0.549206   0.581379  -0.717649  -0.724723 -0.661560   \n",
       "3    1.530221 -1.104676  -0.729603  -0.948326   0.902732   0.876454 -0.703616   \n",
       "4   -0.561197 -0.116520  -0.371537  -0.402003   0.200567   0.247420 -0.390105   \n",
       "5   -0.038343  0.130519   0.702663   0.799908  -0.879687  -0.781908 -0.141590   \n",
       "..        ...       ...        ...        ...        ...        ...       ...   \n",
       "596  0.484512 -0.610598   0.191139   0.144320  -0.231534  -0.267244 -0.600387   \n",
       "597  0.484512 -1.598753  -0.524994  -0.620532   0.470631   0.476160 -0.745673   \n",
       "598 -0.561197 -0.116520  -0.524994  -0.511267   0.416618   0.418975 -0.462748   \n",
       "599 -0.038343 -1.351714   0.088835   0.144320  -0.339560  -0.267244 -0.703616   \n",
       "600  1.530221 -0.610598   0.958425   0.963805  -0.987712  -1.010648 -0.550684   \n",
       "\n",
       "         pscd target  \n",
       "id                    \n",
       "1   -0.585915      0  \n",
       "2    0.370583      0  \n",
       "3    0.812835      0  \n",
       "4   -0.498493      0  \n",
       "5   -0.735047      0  \n",
       "..        ...    ...  \n",
       "596 -0.092239      1  \n",
       "597  2.684692      1  \n",
       "598 -0.524206      1  \n",
       "599  0.910542      0  \n",
       "600 -0.380217      0  \n",
       "\n",
       "[600 rows x 36 columns]"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "      ..\n",
       "596    1\n",
       "597    1\n",
       "598    1\n",
       "599    0\n",
       "600    0\n",
       "Name: target, Length: 600, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in int_cols:\n",
    "    for c in int_cols:\n",
    "        tmp_train[f'{c}_binding_encoded'] = pd.cut(tmp_train[c], bins=5, labels=[1,2,3,4,5])\n",
    "\n",
    "for i in int_cols:\n",
    "    for c in int_cols:\n",
    "        tmp_test[f'{c}_binding_encoded'] = pd.cut(test[c], bins=5, labels=[1,2,3,4,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train= tmp_train.loc[~((tmp_train['hst_binding_encoded']>=5) & (tmp_train['target']==0 ))]\n",
    "tmp_train= tmp_train.loc[~((tmp_train['hst_binding_encoded']<=1) & (tmp_train['target']==1 ))]\n",
    "tmp_train= tmp_train.loc[~((tmp_train['hs_binding_encoded']>=5) & (tmp_train['target']==0 ))]\n",
    "tmp_train= tmp_train.loc[~((tmp_train['hs_binding_encoded']<=1) & (tmp_train['target']==1 ))]\n",
    "tmp_train= tmp_train.loc[~((tmp_train['hy_binding_encoded']>=4) & (tmp_train['target']==1 ))]\n",
    "tmp_train= tmp_train.loc[~((tmp_train['hy_binding_encoded']<=1) & (tmp_train['target']==0 ))]\n",
    "# tmp_train= tmp_train.loc[~((tmp_train['hr_binding_encoded']>=5) & (tmp_train['target']==1 ))]\n",
    "# tmp_train= tmp_train.loc[~((tmp_train['hr_binding_encoded']<=1) & (tmp_train['target']==0 ))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp_train['ratio_hs'] = data['hst']/data['hs']\n",
    "tmp_train['h_max'] = tmp_train[h_columns[:6]].max(axis=1)\n",
    "tmp_train['h_mean'] = tmp_train[h_columns[:6]].mean(axis=1)\n",
    "tmp_train['h_min'] = tmp_train[h_columns[:6]].min(axis=1)\n",
    "tmp_train['d_max'] = tmp_train[d_columns[:6]].max(axis=1)\n",
    "tmp_train['d_mean'] = tmp_train[d_columns[:6]].mean(axis=1)\n",
    "tmp_train['d_min'] = tmp_train[d_columns[:6]].min(axis=1)\n",
    "\n",
    "tmp_test['h_max'] = tmp_test[h_columns[:6]].max(axis=1)\n",
    "tmp_test['h_mean'] = tmp_test[h_columns[:6]].mean(axis=1)\n",
    "tmp_test['h_min'] = tmp_test[h_columns[:6]].min(axis=1)\n",
    "tmp_test['d_max'] = tmp_test[d_columns[:6]].max(axis=1)\n",
    "tmp_test['d_mean'] = tmp_test[d_columns[:6]].mean(axis=1)\n",
    "tmp_test['d_min'] = tmp_test[d_columns[:6]].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "# win = tmp_train['h_max']\n",
    "# draw = tmp_train['d_max']\n",
    "# tmp_train['loss_max'] = abs(1 / (1 / win + 1 / draw))\n",
    "\n",
    "# win = tmp_train['h_min']\n",
    "# draw = tmp_train['d_min']\n",
    "# tmp_train['loss_min'] = 1 / (1 / win + 1 / draw)\n",
    "\n",
    "# win = tmp_train['h_mean']\n",
    "# draw = tmp_train['d_mean']\n",
    "# tmp_train['loss_mean'] = abs(1 / (1 / win + 1 / draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1      0\n",
       "5      0\n",
       "7      1\n",
       "8      0\n",
       "14     1\n",
       "      ..\n",
       "595    1\n",
       "596    1\n",
       "597    1\n",
       "598    1\n",
       "599    0\n",
       "Name: target, Length: 365, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "1      0\n",
       "5      0\n",
       "7      1\n",
       "8      0\n",
       "14     1\n",
       "      ..\n",
       "595    1\n",
       "596    1\n",
       "597    1\n",
       "598    1\n",
       "599    0\n",
       "Name: target, Length: 365, dtype: category\n",
       "Categories (2, int64): [0, 1]"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train['target'] = (tmp_train['target']).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/n9_2kr5s6nx_5b2y_kx3mnnm0000gn/T/ipykernel_86233/3242777858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "abs(tmp_train.corr()['target']).sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = tmp_train.drop(or_cols,axis=1)\n",
    "tmp = tmp_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65        60\n",
      "           1       0.65      0.68      0.67        60\n",
      "\n",
      "    accuracy                           0.66       120\n",
      "   macro avg       0.66      0.66      0.66       120\n",
      "weighted avg       0.66      0.66      0.66       120\n",
      " 0.6581196581196581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.88      0.78        60\n",
      "           1       0.84      0.62      0.71        60\n",
      "\n",
      "    accuracy                           0.75       120\n",
      "   macro avg       0.77      0.75      0.75       120\n",
      "weighted avg       0.77      0.75      0.75       120\n",
      " 0.745475113122172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.68        60\n",
      "           1       0.68      0.57      0.62        60\n",
      "\n",
      "    accuracy                           0.65       120\n",
      "   macro avg       0.65      0.65      0.65       120\n",
      "weighted avg       0.65      0.65      0.65       120\n",
      " 0.6475524475524476\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.77      0.74        60\n",
      "           1       0.75      0.68      0.71        60\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.73      0.73      0.72       120\n",
      "weighted avg       0.73      0.72      0.72       120\n",
      " 0.7245217391304348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74        60\n",
      "           1       0.75      0.73      0.74        60\n",
      "\n",
      "    accuracy                           0.74       120\n",
      "   macro avg       0.74      0.74      0.74       120\n",
      "weighted avg       0.74      0.74      0.74       120\n",
      " 0.7416487256059449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6581196581196581,\n",
       " 0.745475113122172,\n",
       " 0.6475524475524476,\n",
       " 0.7245217391304348,\n",
       " 0.7416487256059449]"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [i for i in tmp.columns if i != 'target']\n",
    "cross_validation(data[or_cols], data['target'],discriminant_analysis.LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7034635367061315"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([0.6581196581196581,\n",
    " 0.745475113122172,\n",
    " 0.6475524475524476,\n",
    " 0.7245217391304348,\n",
    " 0.7416487256059449])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    219\n",
       "0    146\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BalanceCascade' from 'imblearn.ensemble' (/opt/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/n9_2kr5s6nx_5b2y_kx3mnnm0000gn/T/ipykernel_86233/2755777768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBalanceCascade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m bc = BalanceCascade(random_state=0,\n\u001b[1;32m      4\u001b[0m                     \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     n_max_subset=10)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BalanceCascade' from 'imblearn.ensemble' (/opt/anaconda3/lib/python3.9/site-packages/imblearn/ensemble/__init__.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import BalanceCascade\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "bc = BalanceCascade(random_state=0,\n",
    "                    estimator=LogisticRegression(random_state=0),\n",
    "                    n_max_subset=10)\n",
    "\n",
    "\n",
    "bc.fit(tmp[X], tmp['target'])\n",
    "X_resampled, y_resampled = bc.sample(tmp[X], tmp['target'])\n",
    "colors = ['#ef8a62' if v == 0 else '#f7f7f7' if v == 1 else '#67a9cf' for v in y_resampled[0, :]]\n",
    "plt.scatter(X_resampled[0, :, 0], X_resampled[0, :, 1], c=colors, linewidth=1, edgecolor='black')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tmp[X], tmp['target'], test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.821917808219178"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = discriminant_analysis.LinearDiscriminantAnalysis(solver='svd').fit(X_train, y_train)\n",
    "#clf  = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = discriminant_analysis.LinearDiscriminantAnalysis(solver='svd').fit(tmp[X], tmp['target'])\n",
    "clf.predict(tmp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 0\n",
      " 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1]\n",
      "[0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 1 1]\n",
      "33\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(pred, np.array(y_test),sep='\\n')\n",
    "print(pred.sum(), np.array(y_test).sum(),sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 73)"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.sum(), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32,  8],\n",
       "       [ 5, 28]])"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(pred,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
