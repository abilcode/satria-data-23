{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn import linear_model, tree, discriminant_analysis, naive_bayes, ensemble, gaussian_process\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, FunctionTransformer, LabelEncoder, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from pycaret.classification import *\n",
    "from function.data_engineering import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('penyisihan-satria-data-itb-2023/train.csv')\n",
    "test = pd.read_csv('penyisihan-satria-data-itb-2023/test.csv')\n",
    "data.index = data.id\n",
    "test.index = test.id\n",
    "data = data.iloc[:,1:]\n",
    "test = test.iloc[:,1:]\n",
    "or_cols = [c for c in data.columns if c != 'target']\n",
    "int_cols = [c for c in data.columns if data[c].dtype=='int64' and c != 'target']\n",
    "float_cols = [c for c in data.columns if data[c].dtype=='float64' and c != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_columns = [c for c in data.columns if c[-1]=='h']\n",
    "d_columns = [c for c in data.columns if c[-1]=='d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bb_a_hh        0.409673\n",
       "hst            0.404691\n",
       "iwh            0.320410\n",
       "psch           0.319234\n",
       "psh            0.312713\n",
       "bb_av_h        0.309727\n",
       "vch            0.307735\n",
       "bwh            0.307491\n",
       "whh            0.303591\n",
       "b365h          0.303146\n",
       "bb_mx_h        0.298787\n",
       "bb_av_2_5_1    0.257839\n",
       "iwd            0.256921\n",
       "bb_mx_2_5_1    0.255355\n",
       "whd            0.252662\n",
       "bb_av_d        0.249135\n",
       "bwd            0.247222\n",
       "b365d          0.246322\n",
       "vcd            0.245664\n",
       "pscd           0.245553\n",
       "psd            0.244675\n",
       "bb_mx_d        0.244367\n",
       "bb_av_2_5      0.208117\n",
       "bb_mx_2_5      0.204625\n",
       "hs             0.203025\n",
       "hf             0.154112\n",
       "hy             0.125226\n",
       "bb_ou          0.123818\n",
       "hr             0.121942\n",
       "bb_ah          0.073200\n",
       "bb_mx_ahh      0.055756\n",
       "bb_av_ahh      0.045254\n",
       "bb_av_aha      0.034883\n",
       "bb_mx_aha      0.026466\n",
       "hc             0.013607\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(data.corr()['target']).sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b365h</th>\n",
       "      <th>bwh</th>\n",
       "      <th>iwh</th>\n",
       "      <th>psh</th>\n",
       "      <th>whh</th>\n",
       "      <th>vch</th>\n",
       "      <th>bb_mx_h</th>\n",
       "      <th>bb_av_h</th>\n",
       "      <th>bb_ah</th>\n",
       "      <th>bb_a_hh</th>\n",
       "      <th>bb_mx_ahh</th>\n",
       "      <th>bb_av_ahh</th>\n",
       "      <th>psch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.95</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.94</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.28</td>\n",
       "      <td>1.26</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.28</td>\n",
       "      <td>20</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.28</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.27</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.78</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1.83</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.89</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.47</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1.55</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.57</td>\n",
       "      <td>21</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1.16</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.17</td>\n",
       "      <td>21</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1.90</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.89</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.86</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.24</td>\n",
       "      <td>20</td>\n",
       "      <td>-1.75</td>\n",
       "      <td>2.03</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1.66</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.65</td>\n",
       "      <td>23</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     b365h   bwh   iwh   psh   whh   vch  bb_mx_h  bb_av_h  bb_ah  bb_a_hh  \\\n",
       "id                                                                           \n",
       "1     1.95  1.91  1.90  1.93  1.85  1.95     2.02     1.94     22    -0.75   \n",
       "2     1.28  1.26  1.27  1.30  1.29  1.29     1.32     1.28     20    -1.75   \n",
       "3     1.28  1.28  1.33  1.27  1.25  1.29     1.33     1.27     23    -1.50   \n",
       "4     1.90  1.83  1.90  1.91  1.91  1.90     1.98     1.89     19    -0.50   \n",
       "5     2.50  2.50  2.50  2.52  2.50  2.50     2.57     2.47     20    -0.25   \n",
       "..     ...   ...   ...   ...   ...   ...      ...      ...    ...      ...   \n",
       "596   1.55  1.57  1.50  1.58  1.55  1.57     1.60     1.57     21    -1.00   \n",
       "597   1.16  1.16  1.22  1.18  1.17  1.18     1.22     1.17     21    -2.00   \n",
       "598   1.90  1.90  1.95  1.91  1.91  1.85     1.95     1.89     19    -0.50   \n",
       "599   1.25  1.25  1.25  1.24  1.25  1.25     1.30     1.24     20    -1.75   \n",
       "600   1.66  1.65  1.70  1.65  1.63  1.67     1.70     1.65     23    -1.00   \n",
       "\n",
       "     bb_mx_ahh  bb_av_ahh  psch  \n",
       "id                               \n",
       "1         2.26       2.21  1.96  \n",
       "2         2.12       2.06  1.37  \n",
       "3         1.87       1.78  1.26  \n",
       "4         1.94       1.88  2.08  \n",
       "5         2.15       2.10  2.73  \n",
       "..         ...        ...   ...  \n",
       "596       2.05       1.98  1.53  \n",
       "597       1.91       1.84  1.15  \n",
       "598       1.91       1.86  1.89  \n",
       "599       2.03       1.98  1.26  \n",
       "600       2.20       2.13  1.66  \n",
       "\n",
       "[600 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[h_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratio_hs</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ratio_hs</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.31859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.31859</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ratio_hs   target\n",
       "ratio_hs   1.00000  0.31859\n",
       "target     0.31859  1.00000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = data.copy()\n",
    "tmp['ratio_hs'] = data['hst']/data['hs']\n",
    "tmp[['ratio_hs','target']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bb_a_hh        0.409673\n",
       "hst            0.404691\n",
       "h_min          0.320534\n",
       "iwh            0.320410\n",
       "psch           0.319234\n",
       "ratio_hs       0.318590\n",
       "psh            0.312713\n",
       "h_mean         0.309952\n",
       "bb_av_h        0.309727\n",
       "vch            0.307735\n",
       "bwh            0.307491\n",
       "whh            0.303591\n",
       "b365h          0.303146\n",
       "h_max          0.301330\n",
       "bb_mx_h        0.298787\n",
       "bb_av_2_5_1    0.257839\n",
       "iwd            0.256921\n",
       "bb_mx_2_5_1    0.255355\n",
       "d_min          0.254191\n",
       "whd            0.252662\n",
       "d_mean         0.249979\n",
       "bb_av_d        0.249135\n",
       "bwd            0.247222\n",
       "b365d          0.246322\n",
       "vcd            0.245664\n",
       "pscd           0.245553\n",
       "psd            0.244675\n",
       "bb_mx_d        0.244367\n",
       "d_max          0.243256\n",
       "bb_av_2_5      0.208117\n",
       "bb_mx_2_5      0.204625\n",
       "hs             0.203025\n",
       "hf             0.154112\n",
       "hy             0.125226\n",
       "bb_ou          0.123818\n",
       "hr             0.121942\n",
       "bb_ah          0.073200\n",
       "bb_mx_ahh      0.055756\n",
       "bb_av_ahh      0.045254\n",
       "bb_av_aha      0.034883\n",
       "bb_mx_aha      0.026466\n",
       "hc             0.013607\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['h_max'] = tmp[h_columns[:6]].max(axis=1)\n",
    "tmp['h_mean'] = tmp[h_columns[:6]].mean(axis=1)\n",
    "tmp['h_min'] = tmp[h_columns[:6]].min(axis=1)\n",
    "tmp['d_max'] = tmp[d_columns[:6]].max(axis=1)\n",
    "tmp['d_mean'] = tmp[d_columns[:6]].mean(axis=1)\n",
    "tmp['d_min'] = tmp[d_columns[:6]].min(axis=1)\n",
    "abs(tmp.corr()['target']).sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [c for c in tmp.columns if c != 'target']\n",
    "scale = MinMaxScaler()\n",
    "scale.fit(tmp[X])\n",
    "tmp[X] = scale.transform(tmp[X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "\n",
    "    #GLM\n",
    "    linear_model.LogisticRegressionCV(),\n",
    "\n",
    "    #Navies Bayes\n",
    "    naive_bayes.BernoulliNB(),\n",
    "    naive_bayes.GaussianNB(),\n",
    "\n",
    "    #Discriminant Analysis\n",
    "    discriminant_analysis.LinearDiscriminantAnalysis(),\n",
    "    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    #Ensemble Methods\n",
    "    ensemble.AdaBoostClassifier(),\n",
    "    ensemble.BaggingClassifier(),\n",
    "    ensemble.ExtraTreesClassifier(),\n",
    "    ensemble.GradientBoostingClassifier(),\n",
    "    ensemble.RandomForestClassifier(),\n",
    "\n",
    "    #Gaussian Processes\n",
    "    gaussian_process.GaussianProcessClassifier(),\n",
    "\n",
    "    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n",
    "#     xgb.XGBClassifier()    \n",
    "]\n",
    "\n",
    "def find_best_algorithms(classifier_list, X, y):\n",
    "    # This function is adapted from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "    # Cross validate model with Kfold stratified cross validation\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "    # Grab the cross validation scores for each algorithm\n",
    "    cv_results = [cross_val_score(classifier, X, y, scoring = \"accuracy\", cv = kfold) for classifier in classifier_list]\n",
    "    cv_means = [cv_result.mean() * -1 for cv_result in cv_results]\n",
    "    cv_std = [cv_result.std() for cv_result in cv_results]\n",
    "    algorithm_names = [alg.__class__.__name__ for alg in classifiers]\n",
    "\n",
    "    # Create a DataFrame of all the CV results\n",
    "    cv_results = pd.DataFrame({\n",
    "        \"Accuracy\": cv_means,\n",
    "        \"Accuracy Std\": cv_std,\n",
    "        \"Algorithm\": algorithm_names\n",
    "    }).sort_values(by='Accuracy')\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy Std</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.716667</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>LinearDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.703333</td>\n",
       "      <td>0.048477</td>\n",
       "      <td>LogisticRegressionCV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.691667</td>\n",
       "      <td>0.045031</td>\n",
       "      <td>GaussianProcessClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.675000</td>\n",
       "      <td>0.044721</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.671667</td>\n",
       "      <td>0.034801</td>\n",
       "      <td>BaggingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.671667</td>\n",
       "      <td>0.048762</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.670000</td>\n",
       "      <td>0.038944</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.658333</td>\n",
       "      <td>0.051908</td>\n",
       "      <td>ExtraTreesClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.640000</td>\n",
       "      <td>0.037417</td>\n",
       "      <td>GaussianNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.615000</td>\n",
       "      <td>0.021985</td>\n",
       "      <td>QuadraticDiscriminantAnalysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.503333</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>BernoulliNB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Accuracy  Accuracy Std                      Algorithm\n",
       "3  -0.716667      0.038370     LinearDiscriminantAnalysis\n",
       "0  -0.703333      0.048477           LogisticRegressionCV\n",
       "10 -0.691667      0.045031      GaussianProcessClassifier\n",
       "8  -0.675000      0.044721     GradientBoostingClassifier\n",
       "6  -0.671667      0.034801              BaggingClassifier\n",
       "9  -0.671667      0.048762         RandomForestClassifier\n",
       "5  -0.670000      0.038944             AdaBoostClassifier\n",
       "7  -0.658333      0.051908           ExtraTreesClassifier\n",
       "2  -0.640000      0.037417                     GaussianNB\n",
       "4  -0.615000      0.021985  QuadraticDiscriminantAnalysis\n",
       "1  -0.503333      0.011304                    BernoulliNB"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ['bb_a_hh','hst','h_min','iwh','psch','ratio_hs','psh','h_mean','bb_av_h','vch','bwh','whh','b365h','h_max','bb_mx_h','bb_av_2_5_1','iwd','bb_mx_2_5_1','d_min','whd','d_mean','bb_av_d','bwd','b365d']\n",
    "algorithm_results = find_best_algorithms(classifiers, tmp.loc[:,var], tmp.loc[:,'target'])\n",
    "algorithm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_057e4_row8_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_057e4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_057e4_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_057e4_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_057e4_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_057e4_row0_col1\" class=\"data row0 col1\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_057e4_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_057e4_row1_col1\" class=\"data row1 col1\" >target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_057e4_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_057e4_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_057e4_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_057e4_row3_col1\" class=\"data row3 col1\" >(600, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_057e4_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_057e4_row4_col1\" class=\"data row4 col1\" >(600, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_057e4_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_057e4_row5_col1\" class=\"data row5 col1\" >(420, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_057e4_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_057e4_row6_col1\" class=\"data row6 col1\" >(180, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_057e4_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_057e4_row7_col1\" class=\"data row7 col1\" >24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_057e4_row8_col0\" class=\"data row8 col0\" >Preprocess</td>\n",
       "      <td id=\"T_057e4_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_057e4_row9_col0\" class=\"data row9 col0\" >Imputation type</td>\n",
       "      <td id=\"T_057e4_row9_col1\" class=\"data row9 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_057e4_row10_col0\" class=\"data row10 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_057e4_row10_col1\" class=\"data row10 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_057e4_row11_col0\" class=\"data row11 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_057e4_row11_col1\" class=\"data row11 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_057e4_row12_col0\" class=\"data row12 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_057e4_row12_col1\" class=\"data row12 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_057e4_row13_col0\" class=\"data row13 col0\" >Fold Number</td>\n",
       "      <td id=\"T_057e4_row13_col1\" class=\"data row13 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_057e4_row14_col0\" class=\"data row14 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_057e4_row14_col1\" class=\"data row14 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_057e4_row15_col0\" class=\"data row15 col0\" >Use GPU</td>\n",
       "      <td id=\"T_057e4_row15_col1\" class=\"data row15 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_057e4_row16_col0\" class=\"data row16 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_057e4_row16_col1\" class=\"data row16 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_057e4_row17_col0\" class=\"data row17 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_057e4_row17_col1\" class=\"data row17 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_057e4_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_057e4_row18_col0\" class=\"data row18 col0\" >USI</td>\n",
       "      <td id=\"T_057e4_row18_col1\" class=\"data row18 col1\" >eb60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbd48a52970>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pycaret.classification.oop.ClassificationExperiment at 0x7fbd0d34a8b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_ = var.copy()\n",
    "var_.append('target')\n",
    "exp_name = setup(data = tmp[var_], target = 'target',fold=5,session_id = 42, )\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bb_a_hh',\n",
       " 'hst',\n",
       " 'h_min',\n",
       " 'iwh',\n",
       " 'psch',\n",
       " 'ratio_hs',\n",
       " 'psh',\n",
       " 'h_mean',\n",
       " 'bb_av_h',\n",
       " 'vch',\n",
       " 'bwh',\n",
       " 'whh',\n",
       " 'b365h',\n",
       " 'h_max',\n",
       " 'bb_mx_h',\n",
       " 'bb_av_2_5_1',\n",
       " 'iwd',\n",
       " 'bb_mx_2_5_1',\n",
       " 'd_min',\n",
       " 'whd',\n",
       " 'd_mean',\n",
       " 'bb_av_d',\n",
       " 'bwd',\n",
       " 'b365d',\n",
       " 'target']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.68      0.69        60\n",
      "           1       0.69      0.70      0.69        60\n",
      "\n",
      "    accuracy                           0.69       120\n",
      "   macro avg       0.69      0.69      0.69       120\n",
      "weighted avg       0.69      0.69      0.69       120\n",
      " 0.6916452531425793\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.87      0.76        60\n",
      "           1       0.82      0.60      0.69        60\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.75      0.73      0.73       120\n",
      "weighted avg       0.75      0.73      0.73       120\n",
      " 0.7285067873303168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69        60\n",
      "           1       0.70      0.58      0.64        60\n",
      "\n",
      "    accuracy                           0.67       120\n",
      "   macro avg       0.67      0.67      0.66       120\n",
      "weighted avg       0.67      0.67      0.66       120\n",
      " 0.6643356643356644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72        60\n",
      "           1       0.72      0.70      0.71        60\n",
      "\n",
      "    accuracy                           0.72       120\n",
      "   macro avg       0.72      0.72      0.72       120\n",
      "weighted avg       0.72      0.72      0.72       120\n",
      " 0.7165879410947485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75        60\n",
      "           1       0.74      0.77      0.75        60\n",
      "\n",
      "    accuracy                           0.75       120\n",
      "   macro avg       0.75      0.75      0.75       120\n",
      "weighted avg       0.75      0.75      0.75       120\n",
      " 0.7499305362600722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6916452531425793,\n",
       " 0.7285067873303168,\n",
       " 0.6643356643356644,\n",
       " 0.7165879410947485,\n",
       " 0.7499305362600722]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(tmp.loc[:,X],tmp['target'],discriminant_analysis.LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68        60\n",
      "           1       0.69      0.40      0.51        60\n",
      "\n",
      "    accuracy                           0.61       120\n",
      "   macro avg       0.63      0.61      0.59       120\n",
      "weighted avg       0.63      0.61      0.59       120\n",
      " 0.590562613430127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.66        60\n",
      "           1       0.66      0.70      0.68        60\n",
      "\n",
      "    accuracy                           0.67       120\n",
      "   macro avg       0.67      0.67      0.67       120\n",
      "weighted avg       0.67      0.67      0.67       120\n",
      " 0.6662958843159066\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.80      0.70        60\n",
      "           1       0.71      0.50      0.59        60\n",
      "\n",
      "    accuracy                           0.65       120\n",
      "   macro avg       0.66      0.65      0.64       120\n",
      "weighted avg       0.66      0.65      0.64       120\n",
      " 0.6419437340153452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64        60\n",
      "           1       0.65      0.70      0.67        60\n",
      "\n",
      "    accuracy                           0.66       120\n",
      "   macro avg       0.66      0.66      0.66       120\n",
      "weighted avg       0.66      0.66      0.66       120\n",
      " 0.6577391304347826\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.83      0.74        60\n",
      "           1       0.78      0.58      0.67        60\n",
      "\n",
      "    accuracy                           0.71       120\n",
      "   macro avg       0.72      0.71      0.70       120\n",
      "weighted avg       0.72      0.71      0.70       120\n",
      " 0.7037037037037037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.590562613430127,\n",
       " 0.6662958843159066,\n",
       " 0.6419437340153452,\n",
       " 0.6577391304347826,\n",
       " 0.7037037037037037]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cross_validation(data.iloc[:,:-1],data['target'],naive_bayes.GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69        60\n",
      "           1       0.70      0.53      0.60        60\n",
      "\n",
      "    accuracy                           0.65       120\n",
      "   macro avg       0.66      0.65      0.65       120\n",
      "weighted avg       0.66      0.65      0.65       120\n",
      " 0.6451703745423825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.88      0.77        60\n",
      "           1       0.83      0.58      0.69        60\n",
      "\n",
      "    accuracy                           0.73       120\n",
      "   macro avg       0.76      0.73      0.73       120\n",
      "weighted avg       0.76      0.73      0.73       120\n",
      " 0.7271952259164536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.82      0.71        60\n",
      "           1       0.73      0.50      0.59        60\n",
      "\n",
      "    accuracy                           0.66       120\n",
      "   macro avg       0.68      0.66      0.65       120\n",
      "weighted avg       0.68      0.66      0.65       120\n",
      " 0.6495476885818079\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.88      0.74        60\n",
      "           1       0.81      0.50      0.62        60\n",
      "\n",
      "    accuracy                           0.69       120\n",
      "   macro avg       0.72      0.69      0.68       120\n",
      "weighted avg       0.72      0.69      0.68       120\n",
      " 0.6799077211448344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.83      0.76        60\n",
      "           1       0.80      0.65      0.72        60\n",
      "\n",
      "    accuracy                           0.74       120\n",
      "   macro avg       0.75      0.74      0.74       120\n",
      "weighted avg       0.75      0.74      0.74       120\n",
      " 0.7394775544505917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6451703745423825,\n",
       " 0.7271952259164536,\n",
       " 0.6495476885818079,\n",
       " 0.6799077211448344,\n",
       " 0.7394775544505917]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation(data.iloc[:,:-1],data['target'],SVC(gamma='scale',kernel='rbf',random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bb_a_hh</th>\n",
       "      <th>hst</th>\n",
       "      <th>h_min</th>\n",
       "      <th>iwh</th>\n",
       "      <th>psch</th>\n",
       "      <th>ratio_hs</th>\n",
       "      <th>psh</th>\n",
       "      <th>h_mean</th>\n",
       "      <th>bb_av_h</th>\n",
       "      <th>vch</th>\n",
       "      <th>bwh</th>\n",
       "      <th>whh</th>\n",
       "      <th>b365h</th>\n",
       "      <th>h_max</th>\n",
       "      <th>bb_mx_h</th>\n",
       "      <th>bb_av_2_5_1</th>\n",
       "      <th>iwd</th>\n",
       "      <th>bb_mx_2_5_1</th>\n",
       "      <th>d_min</th>\n",
       "      <th>whd</th>\n",
       "      <th>d_mean</th>\n",
       "      <th>bb_av_d</th>\n",
       "      <th>bwd</th>\n",
       "      <th>b365d</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.049025</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.042074</td>\n",
       "      <td>0.044468</td>\n",
       "      <td>0.044489</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.043108</td>\n",
       "      <td>0.044568</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.034918</td>\n",
       "      <td>0.137184</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.046289</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.013370</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.407942</td>\n",
       "      <td>0.314917</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.275496</td>\n",
       "      <td>0.259615</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.011142</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.404332</td>\n",
       "      <td>0.281768</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.266388</td>\n",
       "      <td>0.263112</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.049025</td>\n",
       "      <td>0.042616</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.043252</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>0.047911</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.038304</td>\n",
       "      <td>0.033432</td>\n",
       "      <td>0.238267</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.063760</td>\n",
       "      <td>0.064685</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.056429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.085546</td>\n",
       "      <td>0.084465</td>\n",
       "      <td>0.070042</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.070939</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.071284</td>\n",
       "      <td>0.072682</td>\n",
       "      <td>0.072682</td>\n",
       "      <td>0.080780</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>0.066119</td>\n",
       "      <td>0.055349</td>\n",
       "      <td>0.061372</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.051515</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.019409</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>0.025621</td>\n",
       "      <td>0.025784</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.162455</td>\n",
       "      <td>0.149171</td>\n",
       "      <td>0.142424</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.098552</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.392265</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.403464</td>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.414414</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.051979</td>\n",
       "      <td>0.034599</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>0.047911</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.011142</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.249097</td>\n",
       "      <td>0.337017</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.313187</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.290130</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.037212</td>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.028376</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.029828</td>\n",
       "      <td>0.031078</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.032312</td>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.184116</td>\n",
       "      <td>0.093923</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.081678</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bb_a_hh       hst     h_min       iwh      psch  ratio_hs       psh  \\\n",
       "id                                                                          \n",
       "1    0.409091  0.400000  0.047198  0.049025  0.037553  0.260870  0.042074   \n",
       "2    0.227273  0.333333  0.012389  0.011813  0.012658  0.238095  0.011252   \n",
       "3    0.272727  0.400000  0.011799  0.015357  0.008017  0.352941  0.009785   \n",
       "4    0.454545  0.733333  0.046018  0.049025  0.042616  0.423077  0.041096   \n",
       "5    0.500000  0.400000  0.085546  0.084465  0.070042  0.300000  0.070939   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596  0.363636  0.333333  0.026549  0.025399  0.019409  0.312500  0.024951   \n",
       "597  0.181818  0.800000  0.006490  0.008860  0.003376  0.571429  0.005382   \n",
       "598  0.454545  0.533333  0.047198  0.051979  0.034599  0.533333  0.041096   \n",
       "599  0.227273  0.400000  0.011209  0.010632  0.008017  0.260870  0.008317   \n",
       "600  0.363636  0.333333  0.034218  0.037212  0.024895  0.161290  0.028376   \n",
       "\n",
       "       h_mean   bb_av_h       vch       bwh       whh     b365h     h_max  \\\n",
       "id                                                                          \n",
       "1    0.044468  0.044489  0.045113  0.043108  0.044568  0.040565  0.040128   \n",
       "2    0.011464  0.011122  0.012030  0.010526  0.013370  0.010027  0.010488   \n",
       "3    0.011551  0.010617  0.012030  0.011529  0.011142  0.010027  0.011856   \n",
       "4    0.043252  0.041962  0.042607  0.039098  0.047911  0.038286  0.038304   \n",
       "5    0.075126  0.071284  0.072682  0.072682  0.080780  0.065634  0.066119   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596  0.025621  0.025784  0.026065  0.026065  0.027855  0.022334  0.023256   \n",
       "597  0.006080  0.005561  0.006516  0.005514  0.006685  0.004558  0.006840   \n",
       "598  0.043860  0.041962  0.040100  0.042607  0.047911  0.038286  0.040128   \n",
       "599  0.009727  0.009100  0.010025  0.010025  0.011142  0.008660  0.008208   \n",
       "600  0.031179  0.029828  0.031078  0.030075  0.032312  0.027347  0.028728   \n",
       "\n",
       "      bb_mx_h  bb_av_2_5_1       iwd  bb_mx_2_5_1     d_min       whd  \\\n",
       "id                                                                      \n",
       "1    0.034918     0.137184  0.060773     0.121212  0.065934  0.059406   \n",
       "2    0.008915     0.407942  0.314917     0.357576  0.318681  0.306931   \n",
       "3    0.009287     0.404332  0.281768     0.378788  0.285714  0.306931   \n",
       "4    0.033432     0.238267  0.060773     0.212121  0.065934  0.084158   \n",
       "5    0.055349     0.061372  0.016575     0.051515  0.021978  0.019802   \n",
       "..        ...          ...       ...          ...       ...       ...   \n",
       "596  0.019316     0.162455  0.149171     0.142424  0.120879  0.108911   \n",
       "597  0.005201     0.501805  0.392265     0.424242  0.395604  0.405941   \n",
       "598  0.032318     0.083032  0.055249     0.075758  0.032967  0.029703   \n",
       "599  0.008172     0.249097  0.337017     0.218182  0.313187  0.306931   \n",
       "600  0.023031     0.184116  0.093923     0.166667  0.098901  0.089109   \n",
       "\n",
       "       d_mean   bb_av_d       bwd     b365d  target  \n",
       "id                                                   \n",
       "1    0.046289  0.048077  0.054054  0.035714       0  \n",
       "2    0.275496  0.259615  0.279279  0.250000       0  \n",
       "3    0.266388  0.263112  0.256757  0.250000       0  \n",
       "4    0.063760  0.064685  0.076577  0.056429       0  \n",
       "5    0.013140  0.014860  0.018018  0.014286       0  \n",
       "..        ...       ...       ...       ...     ...  \n",
       "596  0.098552  0.095280  0.099099  0.085714       1  \n",
       "597  0.403464  0.395105  0.414414  0.392857       1  \n",
       "598  0.039122  0.042832  0.045045  0.035714       1  \n",
       "599  0.290130  0.269231  0.256757  0.250000       0  \n",
       "600  0.081678  0.086538  0.099099  0.071429       0  \n",
       "\n",
       "[600 rows x 25 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[var_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hs</th>\n",
       "      <th>hst</th>\n",
       "      <th>hf</th>\n",
       "      <th>hc</th>\n",
       "      <th>hy</th>\n",
       "      <th>hr</th>\n",
       "      <th>b365h</th>\n",
       "      <th>b365d</th>\n",
       "      <th>bwh</th>\n",
       "      <th>bwd</th>\n",
       "      <th>iwh</th>\n",
       "      <th>iwd</th>\n",
       "      <th>psh</th>\n",
       "      <th>psd</th>\n",
       "      <th>whh</th>\n",
       "      <th>whd</th>\n",
       "      <th>vch</th>\n",
       "      <th>vcd</th>\n",
       "      <th>bb_mx_h</th>\n",
       "      <th>bb_av_h</th>\n",
       "      <th>bb_mx_d</th>\n",
       "      <th>bb_av_d</th>\n",
       "      <th>bb_ou</th>\n",
       "      <th>bb_mx_2_5</th>\n",
       "      <th>bb_av_2_5</th>\n",
       "      <th>bb_mx_2_5_1</th>\n",
       "      <th>bb_av_2_5_1</th>\n",
       "      <th>bb_ah</th>\n",
       "      <th>bb_a_hh</th>\n",
       "      <th>bb_mx_ahh</th>\n",
       "      <th>bb_av_ahh</th>\n",
       "      <th>bb_mx_aha</th>\n",
       "      <th>bb_av_aha</th>\n",
       "      <th>psch</th>\n",
       "      <th>pscd</th>\n",
       "      <th>target</th>\n",
       "      <th>ratio_hs</th>\n",
       "      <th>h_max</th>\n",
       "      <th>h_mean</th>\n",
       "      <th>h_min</th>\n",
       "      <th>d_max</th>\n",
       "      <th>d_mean</th>\n",
       "      <th>d_min</th>\n",
       "      <th>bb_a_hh_encode</th>\n",
       "      <th>hst_encode</th>\n",
       "      <th>h_min_encode</th>\n",
       "      <th>iwh_encode</th>\n",
       "      <th>psch_encode</th>\n",
       "      <th>ratio_hs_encode</th>\n",
       "      <th>psh_encode</th>\n",
       "      <th>h_mean_encode</th>\n",
       "      <th>bb_av_h_encode</th>\n",
       "      <th>vch_encode</th>\n",
       "      <th>bwh_encode</th>\n",
       "      <th>whh_encode</th>\n",
       "      <th>b365h_encode</th>\n",
       "      <th>h_max_encode</th>\n",
       "      <th>bb_mx_h_encode</th>\n",
       "      <th>bb_av_2_5_1_encode</th>\n",
       "      <th>iwd_encode</th>\n",
       "      <th>bb_mx_2_5_1_encode</th>\n",
       "      <th>d_min_encode</th>\n",
       "      <th>whd_encode</th>\n",
       "      <th>d_mean_encode</th>\n",
       "      <th>bb_av_d_encode</th>\n",
       "      <th>bwd_encode</th>\n",
       "      <th>b365d_encode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.043108</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.049025</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.042074</td>\n",
       "      <td>0.053289</td>\n",
       "      <td>0.044568</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.045113</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.034918</td>\n",
       "      <td>0.044489</td>\n",
       "      <td>0.038055</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.468421</td>\n",
       "      <td>0.491124</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>0.137184</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.522059</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.196850</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.037553</td>\n",
       "      <td>0.041563</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.044468</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.038020</td>\n",
       "      <td>0.046289</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.078333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>0.314917</td>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.265612</td>\n",
       "      <td>0.013370</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.008915</td>\n",
       "      <td>0.011122</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.259615</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.357576</td>\n",
       "      <td>0.407942</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.419118</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.283465</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.012658</td>\n",
       "      <td>0.196176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.246772</td>\n",
       "      <td>0.275496</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.011529</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.015357</td>\n",
       "      <td>0.281768</td>\n",
       "      <td>0.009785</td>\n",
       "      <td>0.260616</td>\n",
       "      <td>0.011142</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.012030</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.263112</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.163158</td>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.378788</td>\n",
       "      <td>0.404332</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.519685</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.267664</td>\n",
       "      <td>0</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.011856</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011799</td>\n",
       "      <td>0.246772</td>\n",
       "      <td>0.266388</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.056429</td>\n",
       "      <td>0.039098</td>\n",
       "      <td>0.076577</td>\n",
       "      <td>0.049025</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.064113</td>\n",
       "      <td>0.047911</td>\n",
       "      <td>0.084158</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.033432</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.059197</td>\n",
       "      <td>0.064685</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.319527</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.238267</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.286765</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.417323</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.042616</td>\n",
       "      <td>0.055694</td>\n",
       "      <td>0</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.038304</td>\n",
       "      <td>0.043252</td>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.052367</td>\n",
       "      <td>0.063760</td>\n",
       "      <td>0.065934</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065634</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.072682</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.084465</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.070939</td>\n",
       "      <td>0.018318</td>\n",
       "      <td>0.080780</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.072682</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.055349</td>\n",
       "      <td>0.071284</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>0.014860</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.668421</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.051515</td>\n",
       "      <td>0.061372</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.259843</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.070042</td>\n",
       "      <td>0.017456</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.066119</td>\n",
       "      <td>0.075126</td>\n",
       "      <td>0.085546</td>\n",
       "      <td>0.010043</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.301667</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.101667</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.058333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.025399</td>\n",
       "      <td>0.149171</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>0.094921</td>\n",
       "      <td>0.027855</td>\n",
       "      <td>0.108911</td>\n",
       "      <td>0.026065</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>0.025784</td>\n",
       "      <td>0.087385</td>\n",
       "      <td>0.095280</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.426316</td>\n",
       "      <td>0.449704</td>\n",
       "      <td>0.142424</td>\n",
       "      <td>0.162455</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.354331</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.019409</td>\n",
       "      <td>0.121363</td>\n",
       "      <td>1</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.025621</td>\n",
       "      <td>0.026549</td>\n",
       "      <td>0.088953</td>\n",
       "      <td>0.098552</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.414414</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.392265</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>0.421316</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.405941</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.383369</td>\n",
       "      <td>0.395105</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.142012</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.501805</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.255639</td>\n",
       "      <td>0.456693</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.003376</td>\n",
       "      <td>0.570241</td>\n",
       "      <td>1</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.403464</td>\n",
       "      <td>0.395604</td>\n",
       "      <td>0.058333</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038286</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.042607</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.051979</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.050791</td>\n",
       "      <td>0.047911</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.040100</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.038055</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.627219</td>\n",
       "      <td>0.075758</td>\n",
       "      <td>0.083032</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.448819</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.034599</td>\n",
       "      <td>0.051538</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.040128</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.047198</td>\n",
       "      <td>0.035868</td>\n",
       "      <td>0.039122</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.076667</td>\n",
       "      <td>0.061667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.021667</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.256757</td>\n",
       "      <td>0.010632</td>\n",
       "      <td>0.337017</td>\n",
       "      <td>0.008317</td>\n",
       "      <td>0.309742</td>\n",
       "      <td>0.011142</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.260042</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.321053</td>\n",
       "      <td>0.313609</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.249097</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.283458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.258967</td>\n",
       "      <td>0.290130</td>\n",
       "      <td>0.313187</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.036667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027347</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.037212</td>\n",
       "      <td>0.093923</td>\n",
       "      <td>0.028376</td>\n",
       "      <td>0.084097</td>\n",
       "      <td>0.032312</td>\n",
       "      <td>0.089109</td>\n",
       "      <td>0.031078</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.023031</td>\n",
       "      <td>0.029828</td>\n",
       "      <td>0.073291</td>\n",
       "      <td>0.086538</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.378947</td>\n",
       "      <td>0.402367</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.184116</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.477941</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.244094</td>\n",
       "      <td>0.258333</td>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.028728</td>\n",
       "      <td>0.031179</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.067432</td>\n",
       "      <td>0.081678</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.023333</td>\n",
       "      <td>0.038333</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           hs       hst        hf        hc        hy   hr     b365h  \\\n",
       "id                                                                     \n",
       "1    0.617647  0.400000  0.238095  0.555556  0.333333  0.0  0.040565   \n",
       "2    0.558824  0.333333  0.142857  0.444444  0.166667  0.0  0.010027   \n",
       "3    0.441176  0.400000  0.285714  0.388889  0.166667  0.0  0.010027   \n",
       "4    0.705882  0.733333  0.476190  0.333333  0.166667  0.0  0.038286   \n",
       "5    0.529412  0.400000  0.428571  0.166667  0.500000  0.0  0.065634   \n",
       "..        ...       ...       ...       ...       ...  ...       ...   \n",
       "596  0.411765  0.333333  0.238095  0.277778  0.000000  0.0  0.022334   \n",
       "597  0.558824  0.800000  0.142857  0.388889  0.166667  0.0  0.004558   \n",
       "598  0.382353  0.533333  0.476190  0.277778  0.500000  0.0  0.038286   \n",
       "599  0.617647  0.400000  0.380952  0.666667  0.500000  0.0  0.008660   \n",
       "600  0.852941  0.333333  0.333333  0.666667  0.000000  0.0  0.027347   \n",
       "\n",
       "        b365d       bwh       bwd       iwh       iwd       psh       psd  \\\n",
       "id                                                                          \n",
       "1    0.035714  0.043108  0.054054  0.049025  0.060773  0.042074  0.053289   \n",
       "2    0.250000  0.010526  0.279279  0.011813  0.314917  0.011252  0.265612   \n",
       "3    0.250000  0.011529  0.256757  0.015357  0.281768  0.009785  0.260616   \n",
       "4    0.056429  0.039098  0.076577  0.049025  0.060773  0.041096  0.064113   \n",
       "5    0.014286  0.072682  0.018018  0.084465  0.016575  0.070939  0.018318   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596  0.085714  0.026065  0.099099  0.025399  0.149171  0.024951  0.094921   \n",
       "597  0.392857  0.005514  0.414414  0.008860  0.392265  0.005382  0.421316   \n",
       "598  0.035714  0.042607  0.045045  0.051979  0.055249  0.041096  0.050791   \n",
       "599  0.250000  0.010025  0.256757  0.010632  0.337017  0.008317  0.309742   \n",
       "600  0.071429  0.030075  0.099099  0.037212  0.093923  0.028376  0.084097   \n",
       "\n",
       "          whh       whd       vch       vcd   bb_mx_h   bb_av_h   bb_mx_d  \\\n",
       "id                                                                          \n",
       "1    0.044568  0.059406  0.045113  0.041667  0.034918  0.044489  0.038055   \n",
       "2    0.013370  0.306931  0.012030  0.250000  0.008915  0.011122  0.242424   \n",
       "3    0.011142  0.306931  0.012030  0.250000  0.009287  0.010617  0.242424   \n",
       "4    0.047911  0.084158  0.042607  0.062500  0.033432  0.041962  0.059197   \n",
       "5    0.080780  0.019802  0.072682  0.016667  0.055349  0.071284  0.009866   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "596  0.027855  0.108911  0.026065  0.083333  0.019316  0.025784  0.087385   \n",
       "597  0.006685  0.405941  0.006516  0.375000  0.005201  0.005561  0.383369   \n",
       "598  0.047911  0.029703  0.040100  0.041667  0.032318  0.041962  0.038055   \n",
       "599  0.011142  0.306931  0.010025  0.291667  0.008172  0.009100  0.260042   \n",
       "600  0.032312  0.089109  0.031078  0.075000  0.023031  0.029828  0.073291   \n",
       "\n",
       "      bb_av_d     bb_ou  bb_mx_2_5  bb_av_2_5  bb_mx_2_5_1  bb_av_2_5_1  \\\n",
       "id                                                                        \n",
       "1    0.048077  0.705882   0.468421   0.491124     0.121212     0.137184   \n",
       "2    0.259615  0.588235   0.168421   0.177515     0.357576     0.407942   \n",
       "3    0.263112  0.529412   0.163158   0.171598     0.378788     0.404332   \n",
       "4    0.064685  0.764706   0.321053   0.319527     0.212121     0.238267   \n",
       "5    0.014860  0.470588   0.668421   0.692308     0.051515     0.061372   \n",
       "..        ...       ...        ...        ...          ...          ...   \n",
       "596  0.095280  0.529412   0.426316   0.449704     0.142424     0.162455   \n",
       "597  0.395105  0.352941   0.136842   0.142012     0.424242     0.501805   \n",
       "598  0.042832  0.764706   0.610526   0.627219     0.075758     0.083032   \n",
       "599  0.269231  0.529412   0.321053   0.313609     0.218182     0.249097   \n",
       "600  0.086538  0.588235   0.378947   0.402367     0.166667     0.184116   \n",
       "\n",
       "     bb_ah   bb_a_hh  bb_mx_ahh  bb_av_ahh  bb_mx_aha  bb_av_aha      psch  \\\n",
       "id                                                                           \n",
       "1      0.8  0.409091   0.522059   0.533835   0.196850   0.216667  0.037553   \n",
       "2      0.6  0.227273   0.419118   0.421053   0.283465   0.300000  0.012658   \n",
       "3      0.9  0.272727   0.235294   0.210526   0.519685   0.533333  0.008017   \n",
       "4      0.5  0.454545   0.286765   0.285714   0.417323   0.441667  0.042616   \n",
       "5      0.6  0.500000   0.441176   0.451128   0.259843   0.291667  0.070042   \n",
       "..     ...       ...        ...        ...        ...        ...       ...   \n",
       "596    0.7  0.363636   0.367647   0.360902   0.354331   0.366667  0.019409   \n",
       "597    0.7  0.181818   0.264706   0.255639   0.456693   0.475000  0.003376   \n",
       "598    0.5  0.454545   0.264706   0.270677   0.448819   0.466667  0.034599   \n",
       "599    0.6  0.227273   0.352941   0.360902   0.338583   0.366667  0.008017   \n",
       "600    0.9  0.363636   0.477941   0.473684   0.244094   0.258333  0.024895   \n",
       "\n",
       "         pscd  target  ratio_hs     h_max    h_mean     h_min     d_max  \\\n",
       "id                                                                        \n",
       "1    0.041563       0  0.260870  0.040128  0.044468  0.047198  0.038020   \n",
       "2    0.196176       0  0.238095  0.010488  0.011464  0.012389  0.246772   \n",
       "3    0.267664       0  0.352941  0.011856  0.011551  0.011799  0.246772   \n",
       "4    0.055694       0  0.423077  0.038304  0.043252  0.046018  0.052367   \n",
       "5    0.017456       0  0.300000  0.066119  0.075126  0.085546  0.010043   \n",
       "..        ...     ...       ...       ...       ...       ...       ...   \n",
       "596  0.121363       1  0.312500  0.023256  0.025621  0.026549  0.088953   \n",
       "597  0.570241       1  0.571429  0.006840  0.006080  0.006490  0.390244   \n",
       "598  0.051538       1  0.533333  0.040128  0.043860  0.047198  0.035868   \n",
       "599  0.283458       0  0.260870  0.008208  0.009727  0.011209  0.258967   \n",
       "600  0.074813       0  0.161290  0.028728  0.031179  0.034218  0.067432   \n",
       "\n",
       "       d_mean     d_min  bb_a_hh_encode  hst_encode  h_min_encode  iwh_encode  \\\n",
       "id                                                                              \n",
       "1    0.046289  0.065934        0.068333    0.108333      0.008333    0.035000   \n",
       "2    0.275496  0.318681        0.035000    0.136667      0.005000    0.013333   \n",
       "3    0.266388  0.285714        0.058333    0.108333      0.025000    0.015000   \n",
       "4    0.063760  0.065934        0.026667    0.025000      0.013333    0.035000   \n",
       "5    0.013140  0.021978        0.301667    0.108333      0.031667    0.023333   \n",
       "..        ...       ...             ...         ...           ...         ...   \n",
       "596  0.098552  0.120879        0.095000    0.136667      0.010000    0.013333   \n",
       "597  0.403464  0.395604        0.058333    0.015000      0.005000    0.010000   \n",
       "598  0.039122  0.032967        0.026667    0.056667      0.008333    0.005000   \n",
       "599  0.290130  0.313187        0.035000    0.108333      0.001667    0.036667   \n",
       "600  0.081678  0.098901        0.095000    0.136667      0.003333    0.018333   \n",
       "\n",
       "     psch_encode  ratio_hs_encode  psh_encode  h_mean_encode  bb_av_h_encode  \\\n",
       "id                                                                             \n",
       "1       0.003333         0.006667    0.005000       0.001667        0.006667   \n",
       "2       0.006667         0.003333    0.008333       0.001667        0.015000   \n",
       "3       0.011667         0.005000    0.015000       0.001667        0.005000   \n",
       "4       0.005000         0.001667    0.003333       0.003333        0.005000   \n",
       "5       0.005000         0.031667    0.003333       0.001667        0.011667   \n",
       "..           ...              ...         ...            ...             ...   \n",
       "596     0.005000         0.008333    0.006667       0.001667        0.005000   \n",
       "597     0.005000         0.005000    0.005000       0.006667        0.003333   \n",
       "598     0.003333         0.006667    0.003333       0.003333        0.005000   \n",
       "599     0.011667         0.006667    0.006667       0.001667        0.015000   \n",
       "600     0.001667         0.001667    0.005000       0.005000        0.008333   \n",
       "\n",
       "     vch_encode  bwh_encode  whh_encode  b365h_encode  h_max_encode  \\\n",
       "id                                                                    \n",
       "1      0.016667    0.010000    0.016667      0.015000      0.006667   \n",
       "2      0.023333    0.016667    0.018333      0.020000      0.013333   \n",
       "3      0.023333    0.011667    0.026667      0.020000      0.011667   \n",
       "4      0.003333    0.010000    0.018333      0.021667      0.005000   \n",
       "5      0.023333    0.018333    0.040000      0.026667      0.001667   \n",
       "..          ...         ...         ...           ...           ...   \n",
       "596    0.005000    0.006667    0.003333      0.006667      0.003333   \n",
       "597    0.018333    0.011667    0.018333      0.013333      0.010000   \n",
       "598    0.013333    0.010000    0.018333      0.021667      0.006667   \n",
       "599    0.026667    0.011667    0.026667      0.025000      0.028333   \n",
       "600    0.015000    0.013333    0.003333      0.016667      0.013333   \n",
       "\n",
       "     bb_mx_h_encode  bb_av_2_5_1_encode  iwd_encode  bb_mx_2_5_1_encode  \\\n",
       "id                                                                        \n",
       "1          0.006667            0.011667    0.070000            0.020000   \n",
       "2          0.005000            0.008333    0.003333            0.003333   \n",
       "3          0.011667            0.001667    0.025000            0.003333   \n",
       "4          0.003333            0.006667    0.070000            0.018333   \n",
       "5          0.003333            0.008333    0.038333            0.018333   \n",
       "..              ...                 ...         ...                 ...   \n",
       "596        0.013333            0.010000    0.006667            0.015000   \n",
       "597        0.011667            0.001667    0.028333            0.003333   \n",
       "598        0.005000            0.016667    0.016667            0.023333   \n",
       "599        0.010000            0.006667    0.040000            0.005000   \n",
       "600        0.011667            0.013333    0.026667            0.016667   \n",
       "\n",
       "     d_min_encode  whd_encode  d_mean_encode  bb_av_d_encode  bwd_encode  \\\n",
       "id                                                                         \n",
       "1        0.076667    0.078333       0.001667        0.010000    0.065000   \n",
       "2        0.005000    0.041667       0.001667        0.001667    0.021667   \n",
       "3        0.028333    0.041667       0.001667        0.001667    0.025000   \n",
       "4        0.076667    0.040000       0.001667        0.011667    0.026667   \n",
       "5        0.101667    0.103333       0.001667        0.010000    0.086667   \n",
       "..            ...         ...            ...             ...         ...   \n",
       "596      0.041667    0.026667       0.001667        0.003333    0.035000   \n",
       "597      0.023333    0.025000       0.001667        0.003333    0.016667   \n",
       "598      0.076667    0.061667       0.001667        0.021667    0.086667   \n",
       "599      0.005000    0.041667       0.001667        0.001667    0.025000   \n",
       "600      0.023333    0.038333       0.001667        0.003333    0.035000   \n",
       "\n",
       "     b365d_encode  \n",
       "id                 \n",
       "1        0.085000  \n",
       "2        0.033333  \n",
       "3        0.033333  \n",
       "4        0.016667  \n",
       "5        0.058333  \n",
       "..            ...  \n",
       "596      0.026667  \n",
       "597      0.013333  \n",
       "598      0.085000  \n",
       "599      0.033333  \n",
       "600      0.053333  \n",
       "\n",
       "[600 rows x 67 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in var:\n",
    "  tmp_train = freq_encoding(tmp, col)\n",
    "tmp_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.701 (0.047)\n"
     ]
    }
   ],
   "source": [
    "steps = [('pca', PCA(n_components=15)), ('m', discriminant_analysis.LinearDiscriminantAnalysis())]\n",
    "model = Pipeline(steps=steps)\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=0)\n",
    "n_scores = cross_val_score(model, tmp_train[var], tmp['target'], scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yt/n9_2kr5s6nx_5b2y_kx3mnnm0000gn/T/ipykernel_82565/3505814776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fit the RandomizedSearchCV object to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Print the best parameters found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1398\u001b[0m         )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m   1401\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m             \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1733\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1734\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "# Define the parameter distributions for randomized search\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 8),\n",
    "    'learning_rate': uniform(0.001, 0.1),\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "}\n",
    "\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_search.fit(tmp[var], tmp['target'])\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Print the best F1 score found\n",
    "print(\"Best F1 score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[var_].to_csv('updated_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
